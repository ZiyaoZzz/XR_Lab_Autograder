{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COGS 160 Auto-Grader Notebook for Architect Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import fitz  # PyMuPDF for PDF parsing\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rubric Scoring Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric = {\n",
    "    \"architect_chosen\": 5,\n",
    "    \"bio_750_words\": 10,\n",
    "    \"bio_structure\": 10,\n",
    "    \"bio_references\": 10,\n",
    "    \"10_buildings_with_images\": 15,\n",
    "    \"image_quality\": 10,\n",
    "    \"image_citations\": 10,\n",
    "    \"personal_bio_photo\": 5,\n",
    "    \"doc_and_slides\": 5,\n",
    "    \"image_relevance\": 10,\n",
    "    \"presentation_polish\": 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/Users/tanishqsingh/Downloads/cogs160submisson1.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Images from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_from_pdf(pdf_path, min_width=1200):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    image_data = []\n",
    "    for page_index in range(len(doc)):\n",
    "        images = doc.get_page_images(page_index)\n",
    "        for img_index, img in enumerate(images):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            img_pil = Image.open(BytesIO(image_bytes))\n",
    "            width, height = img_pil.size\n",
    "            image_data.append({\n",
    "                \"page\": page_index + 1,\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "                \"is_high_res\": width >= min_width\n",
    "            })\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Image Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image_quality(image_data):\n",
    "    total = len(image_data)\n",
    "    high_res_count = sum(1 for img in image_data if img[\"is_high_res\"])\n",
    "    score = int((high_res_count / max(1, total)) * rubric[\"image_quality\"])\n",
    "    return {\n",
    "        \"total_images\": total,\n",
    "        \"high_res_count\": high_res_count,\n",
    "        \"score\": score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Word Count & Structure Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_biography(text):\n",
    "    result = {}\n",
    "    doc = nlp(text)\n",
    "    result[\"word_count\"] = len([token.text for token in doc if token.is_alpha])\n",
    "\n",
    "    required_sections = [\"who they are\", \"studied\", \"first building\", \"significance\", \"influence\"]\n",
    "    section_hits = sum([1 for section in required_sections if section.lower() in text.lower()])\n",
    "    result[\"structure_score\"] = int((section_hits / len(required_sections)) * rubric[\"bio_structure\"])\n",
    "\n",
    "    result[\"score\"] = rubric[\"bio_750_words\"] if result[\"word_count\"] >= 700 else int((result[\"word_count\"] / 750) * rubric[\"bio_750_words\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference & Citation Validator (Inline Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_references_from_text(text):\n",
    "    # Grab all lines that look like APA references (Year + optional URL)\n",
    "    lines = text.split(\"\\n\")\n",
    "    references = []\n",
    "    for line in lines:\n",
    "        if re.search(r\"\\(\\d{4}\\)\", line):  # contains (Year)\n",
    "            if any(x in line.lower() for x in [\"doi\", \"archdaily\", \"e-architect\", \"https://\"]):\n",
    "                references.append(line.strip())\n",
    "    return references\n",
    "\n",
    "def evaluate_references(ref_list):\n",
    "    valid = [ref for ref in ref_list if \"doi\" in ref.lower() or \"archdaily\" in ref.lower()]\n",
    "    return {\n",
    "        \"valid_references\": len(valid),\n",
    "        \"score\": min(len(valid), rubric[\"bio_references\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Final Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scorecard(scores):\n",
    "    total = sum([v[\"score\"] for v in scores.values()])\n",
    "    return {\n",
    "        \"scorecard\": {k: v[\"score\"] for k, v in scores.items()},\n",
    "        \"final_score\": total,\n",
    "        \"grade\": \"A\" if total >= 90 else \"B\" if total >= 80 else \"C\" if total >= 70 else \"D\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run All-in-One Evaluation (PDF Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_autograder(pdf_path):\n",
    "    doc_text = extract_text_from_pdf(pdf_path)\n",
    "    image_data = extract_images_from_pdf(pdf_path)\n",
    "    ref_list = extract_references_from_text(doc_text)\n",
    "\n",
    "    results = {}\n",
    "    results[\"bio\"] = evaluate_biography(doc_text)\n",
    "    results[\"references\"] = evaluate_references(ref_list)\n",
    "    results[\"images\"] = evaluate_image_quality(image_data)\n",
    "    results[\"citations\"] = {\"score\": min(len(ref_list), 10)}  # Basic citation count\n",
    "\n",
    "    return generate_scorecard(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"scorecard\": {\n",
      "    \"bio\": 10,\n",
      "    \"references\": 0,\n",
      "    \"images\": 6,\n",
      "    \"citations\": 0\n",
      "  },\n",
      "  \"final_score\": 16,\n",
      "  \"grade\": \"D\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = run_autograder(pdf_path)\n",
    "print(json.dumps(result, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_setup_llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

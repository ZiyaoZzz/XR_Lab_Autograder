{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Converting vidyalatanvi_LATE_218146_14937211_COGS 160 A1-1.docx → vidyalatanvi_LATE_218146_14937211_COGS 160 A1-1.pdf …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:28<00:00, 28.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Extracting images from mainayardaniel_127050_14924649_COGS 160 Le Corbusier Doc.pdf (56 pages)…\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz       # PyMuPDF\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from docx2pdf import convert\n",
    "\n",
    "def preprocess_docs(input_dir: str):\n",
    "    \"\"\"\n",
    "    Convert all .docx/.doc files in input_dir into PDFs\n",
    "    with the same base filename.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(input_dir):\n",
    "        lower = filename.lower()\n",
    "        if lower.endswith((\".docx\", \".doc\")):\n",
    "            doc_path = os.path.join(input_dir, filename)\n",
    "            pdf_path = os.path.join(\n",
    "                input_dir,\n",
    "                os.path.splitext(filename)[0] + \".pdf\"\n",
    "            )\n",
    "            try:\n",
    "                print(f\"🔧 Converting {filename} → {os.path.basename(pdf_path)} …\")\n",
    "                convert(doc_path, pdf_path)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to convert {filename}: {e}\")\n",
    "\n",
    "def extract_images_from_pdfs(\n",
    "    input_dir: str,\n",
    "    output_dir: str,\n",
    "    min_width: int = 1200,\n",
    "    metadata_filename: str = \"image_metadata.csv\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1. Convert any .doc/.docx to .pdf.\n",
    "    2. Walk through each PDF in input_dir, extract all embedded images,\n",
    "    3. Save images under output_dir/<pdf_basename>/…,\n",
    "    4. Compile a CSV of metadata (file, page, dimensions, resolution, high-res flag).\n",
    "    Returns the metadata DataFrame.\n",
    "    \"\"\"\n",
    "    # Step 1: convert Word docs\n",
    "    preprocess_docs(input_dir)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    records = []\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if not filename.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "\n",
    "        pdf_path    = os.path.join(input_dir, filename)\n",
    "        base_name   = os.path.splitext(filename)[0]\n",
    "        doc         = fitz.open(pdf_path)\n",
    "\n",
    "        # Create subfolder for this PDF’s images\n",
    "        pdf_img_dir = os.path.join(output_dir, base_name)\n",
    "        os.makedirs(pdf_img_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"\\n📄 Extracting images from {filename} ({len(doc)} pages)…\")\n",
    "        for page_idx in tqdm(range(len(doc)), desc=\"Pages\", leave=False):\n",
    "            page   = doc[page_idx]\n",
    "            images = page.get_images(full=True)\n",
    "            if not images:\n",
    "                continue\n",
    "\n",
    "            for img_idx, img_info in enumerate(images, start=1):\n",
    "                xref      = img_info[0]\n",
    "                img_data  = doc.extract_image(xref)\n",
    "                img_bytes = img_data[\"image\"]\n",
    "\n",
    "                try:\n",
    "                    img = Image.open(BytesIO(img_bytes))\n",
    "                except Exception as e:\n",
    "                    print(f\" ⚠️ Couldn’t open image on page {page_idx+1}, idx {img_idx}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                w, h     = img.size\n",
    "                img_name = f\"{base_name}_p{page_idx+1}_img{img_idx}.png\"\n",
    "                img_path = os.path.join(pdf_img_dir, img_name)\n",
    "                img.save(img_path)\n",
    "\n",
    "                records.append({\n",
    "                    \"pdf_file\":    filename,\n",
    "                    \"page\":        page_idx + 1,\n",
    "                    \"image_name\":  img_name,\n",
    "                    \"image_path\":  img_path,\n",
    "                    \"width\":       w,\n",
    "                    \"height\":      h,\n",
    "                    \"resolution\":  f\"{w}x{h}\",\n",
    "                    \"is_high_res\": w >= min_width\n",
    "                })\n",
    "\n",
    "        doc.close()\n",
    "\n",
    "    # Write out metadata CSV\n",
    "    if records:\n",
    "        df       = pd.DataFrame(records)\n",
    "        csv_path = os.path.join(output_dir, metadata_filename)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"\\n✅ Extracted {len(records)} images; metadata saved to {csv_path}\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"⚠️ No images found in any PDF.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ── Example usage ────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    metadata_df = extract_images_from_pdfs(\n",
    "        input_dir=\"submissions\",\n",
    "        output_dir=\"Extracted_images\",\n",
    "        min_width=1200\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Compressing mainayardaniel_127050_14924649_COGS 160 Le Corbusier Doc.pdf ...\n",
      "✅ Saved compressed PDF: compressed_files/mainayardaniel_127050_14924649_COGS 160 Le Corbusier Doc.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz      # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def compress_all_pdfs(input_dir, output_dir, dpi=100, downscale_factor=2):\n",
    "    \"\"\"\n",
    "    Compress all PDF files in `input_dir` by rendering each page to an image,\n",
    "    optionally downscaling, and reassembling into a new PDF in `output_dir`.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if not filename.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        print(f\"🔄 Compressing {filename} ...\")\n",
    "        try:\n",
    "            doc = fitz.open(input_path)\n",
    "            new_pdf = fitz.open()\n",
    "            for page in doc:\n",
    "                pix = page.get_pixmap(dpi=dpi)\n",
    "                img = Image.frombytes(\"RGB\", (pix.width, pix.height), pix.samples)\n",
    "                new_size = (pix.width // downscale_factor, pix.height // downscale_factor)\n",
    "                img = img.resize(new_size, Image.LANCZOS)\n",
    "                buffer = io.BytesIO()\n",
    "                img.save(buffer, format=\"PDF\", resolution=dpi)\n",
    "                buffer.seek(0)\n",
    "                img_pdf = fitz.open(\"pdf\", buffer)\n",
    "                new_pdf.insert_pdf(img_pdf)\n",
    "            new_pdf.save(output_path)\n",
    "            new_pdf.close()\n",
    "            doc.close()\n",
    "            print(f\"✅ Saved compressed PDF: {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to compress {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Unmatched login_ids: ['mainayardaniel' 'emralinolalaine' 'krukjulia' 'spavenchristine' 'khirwadkarisha' 'liangmichael' 'yangheiman' 'dasilvatheo' 'davidmatthew' 'marvanalicia' 'vidyalatanvi']\n",
      "\n",
      "✅ Done! Check → image_metadata_with_name_pid.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# ── 1) Load CSVs ──────────────────────────────────\n",
    "meta_df   = pd.read_csv(\"image_metadata.csv\")\n",
    "roster_df = pd.read_csv(\"student_info.csv\")\n",
    "\n",
    "# ── 2) Extract login_id from PDF filenames ────────\n",
    "meta_df[\"login_id\"] = meta_df[\"pdf_file\"].str.split(\"_\").str[0]\n",
    "\n",
    "# ── 3) Tidy roster columns ────────────────────────\n",
    "roster_df = roster_df.rename(columns={\n",
    "    \"SIS Login ID\": \"login_id\",\n",
    "    \"Student\":      \"student_name\",\n",
    "    \"SIS User ID\":  \"pid\"\n",
    "})\n",
    "roster_df[\"student_name\"] = roster_df[\"student_name\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "# ── 4) Exact-match merge ──────────────────────────\n",
    "merged = pd.merge(\n",
    "    meta_df,\n",
    "    roster_df[[\"login_id\",\"student_name\",\"pid\"]],\n",
    "    on=\"login_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ── 5) Find unmatched IDs ─────────────────────────\n",
    "unmatched = merged.loc[merged[\"pid\"].isna(), \"login_id\"].unique()\n",
    "print(\"🔍 Unmatched login_ids:\", unmatched)\n",
    "\n",
    "# ── 6) Prepare fuzzy matching ─────────────────────\n",
    "roster_df[\"norm_name\"] = (\n",
    "  roster_df[\"student_name\"].str.lower().str.replace(r\"[^a-z0-9]\", \"\", regex=True)\n",
    ")\n",
    "roster_map = { row.norm_name: (row.login_id, row.student_name, row.pid) for row in roster_df.itertuples() }\n",
    "\n",
    "# ── 7) Fuzzy-match suggestions ────────────────────\n",
    "suggestions = {}\n",
    "for uid in unmatched:\n",
    "    key = str(uid).lower()\n",
    "    if key in roster_map:\n",
    "        suggestions[uid] = [roster_map[key]]\n",
    "        continue\n",
    "    hits = [roster_map[n] for n in roster_map if key in n or n in key]\n",
    "    if hits:\n",
    "        suggestions[uid] = hits\n",
    "        continue\n",
    "    best = get_close_matches(key, roster_map.keys(), n=1, cutoff=0.6)\n",
    "    suggestions[uid] = [roster_map[best[0]]] if best else []\n",
    "\n",
    "# ── 8) Auto-fill unique matches ───────────────────\n",
    "for uid, matches in suggestions.items():\n",
    "    if len(matches) == 1:\n",
    "        _, name, pid = matches[0]\n",
    "        merged.loc[merged[\"login_id\"]==uid, \"student_name\"] = name\n",
    "        merged.loc[merged[\"login_id\"]==uid, \"pid\"]          = pid\n",
    "\n",
    "# ── 9) Report ambiguous/no matches ───────────────\n",
    "for uid, matches in suggestions.items():\n",
    "    if len(matches)>1:\n",
    "        print(f\"⚠️ {uid!r} HAS MULTIPLE MATCHES:\")\n",
    "        for _, name, pid in matches:\n",
    "            print(f\"    {name} → {pid}\")\n",
    "    elif not matches:\n",
    "        print(f\"❌ {uid!r} HAS NO CLOSE MATCH\")\n",
    "\n",
    "# ──10) Save result ────────────────────────────────\n",
    "merged.to_csv(\"image_metadata_with_name_pid.csv\", index=False)\n",
    "print(\"✅ Done! Check → image_metadata_with_name_pid.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XR_Lab (py3.11)",
   "language": "python",
   "name": "xr_lab_311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

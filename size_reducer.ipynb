{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Converting vidyalatanvi_LATE_218146_14937211_COGS 160 A1-1.docx ‚Üí vidyalatanvi_LATE_218146_14937211_COGS 160 A1-1.pdf ‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:28<00:00, 28.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Extracting images from mainayardaniel_127050_14924649_COGS 160 Le Corbusier Doc.pdf (56 pages)‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Extracting images from emralinolalaine_LATE_162831_14938886_Glenn Murcutt_ ‚ÄúTouching the Earth Lightly‚Äù.pdf (14 pages)‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Extracting images from krukjulia_LATE_198551_15046680_Eero Saarinen_ Cogs 160 Research Document.pdf (49 pages)‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Extracting images from spavenchristine_LATE_96300_14929508_COGS 160_1 (1).pdf (42 pages)‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Extracting images from khirwadkarisha_166304_14925482_Report.pdf (36 pages)‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Extracting images from liangmichael_188529_14924464_COGS 160 - Docs.pdf (58 pages)‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Extracting images from yangheiman_LATE_190478_14963531_COGS160.pdf (43 pages)‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Extracting images from wucynthia_LATE_167097_15019933_COGS 160.pdf (1 pages)‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Extracting images from khirwadkarisha_166304_14925456_ken yeang cogs 160 in class.pdf (34 pages)‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Extracting images from hsucalvin_166834_14967682_Cogs 160 Slide and Doc Links for Turn-in.pdf (1 pages)‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Extracting images from dasilvatheo_LATE_171930_14930244_HW A1.pdf (46 pages)‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Extracting images from davidmatthew_LATE_134808_14949557_COGS 160_ A1.pdf (43 pages)‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Extracting images from marvanalicia_212624_14925657_A.Marvan_LuisBarragan.pdf (3 pages)‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Extracting images from delacruzrenier_LATE_226065_14930691_Paul Rudolph_ Life, Work, and Enduring Influence.pdf (6 pages)‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Extracting images from vidyalatanvi_LATE_218146_14937211_COGS 160 A1-1.pdf (61 pages)‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Extracting images from liangmichael_188529_14924465_Michael - COGS 160.pdf (103 pages)‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Extracted 913 images; metadata saved to Extracted_images/image_metadata.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz       # PyMuPDF\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from docx2pdf import convert\n",
    "\n",
    "def preprocess_docs(input_dir: str):\n",
    "    \"\"\"\n",
    "    Convert all .docx/.doc files in input_dir into PDFs\n",
    "    with the same base filename.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(input_dir):\n",
    "        lower = filename.lower()\n",
    "        if lower.endswith((\".docx\", \".doc\")):\n",
    "            doc_path = os.path.join(input_dir, filename)\n",
    "            pdf_path = os.path.join(\n",
    "                input_dir,\n",
    "                os.path.splitext(filename)[0] + \".pdf\"\n",
    "            )\n",
    "            try:\n",
    "                print(f\"üîß Converting {filename} ‚Üí {os.path.basename(pdf_path)} ‚Ä¶\")\n",
    "                convert(doc_path, pdf_path)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to convert {filename}: {e}\")\n",
    "\n",
    "def extract_images_from_pdfs(\n",
    "    input_dir: str,\n",
    "    output_dir: str,\n",
    "    min_width: int = 1200,\n",
    "    metadata_filename: str = \"image_metadata.csv\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1. Convert any .doc/.docx to .pdf.\n",
    "    2. Walk through each PDF in input_dir, extract all embedded images,\n",
    "    3. Save images under output_dir/<pdf_basename>/‚Ä¶,\n",
    "    4. Compile a CSV of metadata (file, page, dimensions, resolution, high-res flag).\n",
    "    Returns the metadata DataFrame.\n",
    "    \"\"\"\n",
    "    # Step 1: convert Word docs\n",
    "    preprocess_docs(input_dir)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    records = []\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if not filename.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "\n",
    "        pdf_path    = os.path.join(input_dir, filename)\n",
    "        base_name   = os.path.splitext(filename)[0]\n",
    "        doc         = fitz.open(pdf_path)\n",
    "\n",
    "        # Create subfolder for this PDF‚Äôs images\n",
    "        pdf_img_dir = os.path.join(output_dir, base_name)\n",
    "        os.makedirs(pdf_img_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"\\nüìÑ Extracting images from {filename} ({len(doc)} pages)‚Ä¶\")\n",
    "        for page_idx in tqdm(range(len(doc)), desc=\"Pages\", leave=False):\n",
    "            page   = doc[page_idx]\n",
    "            images = page.get_images(full=True)\n",
    "            if not images:\n",
    "                continue\n",
    "\n",
    "            for img_idx, img_info in enumerate(images, start=1):\n",
    "                xref      = img_info[0]\n",
    "                img_data  = doc.extract_image(xref)\n",
    "                img_bytes = img_data[\"image\"]\n",
    "\n",
    "                try:\n",
    "                    img = Image.open(BytesIO(img_bytes))\n",
    "                except Exception as e:\n",
    "                    print(f\" ‚ö†Ô∏è Couldn‚Äôt open image on page {page_idx+1}, idx {img_idx}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                w, h     = img.size\n",
    "                img_name = f\"{base_name}_p{page_idx+1}_img{img_idx}.png\"\n",
    "                img_path = os.path.join(pdf_img_dir, img_name)\n",
    "                img.save(img_path)\n",
    "\n",
    "                records.append({\n",
    "                    \"pdf_file\":    filename,\n",
    "                    \"page\":        page_idx + 1,\n",
    "                    \"image_name\":  img_name,\n",
    "                    \"image_path\":  img_path,\n",
    "                    \"width\":       w,\n",
    "                    \"height\":      h,\n",
    "                    \"resolution\":  f\"{w}x{h}\",\n",
    "                    \"is_high_res\": w >= min_width\n",
    "                })\n",
    "\n",
    "        doc.close()\n",
    "\n",
    "    # Write out metadata CSV\n",
    "    if records:\n",
    "        df       = pd.DataFrame(records)\n",
    "        csv_path = os.path.join(output_dir, metadata_filename)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"\\n‚úÖ Extracted {len(records)} images; metadata saved to {csv_path}\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No images found in any PDF.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ‚îÄ‚îÄ Example usage ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if __name__ == \"__main__\":\n",
    "    metadata_df = extract_images_from_pdfs(\n",
    "        input_dir=\"submissions\",\n",
    "        output_dir=\"Extracted_images\",\n",
    "        min_width=1200\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docx2pdf\n",
      "  Using cached docx2pdf-0.1.8-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting appscript>=1.1.0 (from docx2pdf)\n",
      "  Downloading appscript-1.3.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (417 bytes)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in ./venv311/lib/python3.11/site-packages (from docx2pdf) (4.67.1)\n",
      "Collecting lxml>=4.7.1 (from appscript>=1.1.0->docx2pdf)\n",
      "  Downloading lxml-5.4.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.5 kB)\n",
      "Using cached docx2pdf-0.1.8-py3-none-any.whl (6.7 kB)\n",
      "Downloading appscript-1.3.0-cp311-cp311-macosx_10_9_universal2.whl (99 kB)\n",
      "Downloading lxml-5.4.0-cp311-cp311-macosx_10_9_universal2.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml, appscript, docx2pdf\n",
      "Successfully installed appscript-1.3.0 docx2pdf-0.1.8 lxml-5.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install docx2pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Compressing mainayardaniel_127050_14924649_COGS 160 Le Corbusier Doc.pdf ...\n",
      "‚úÖ Saved compressed PDF: compressed_files/mainayardaniel_127050_14924649_COGS 160 Le Corbusier Doc.pdf\n",
      "üîÑ Compressing emralinolalaine_LATE_162831_14938886_Glenn Murcutt_ ‚ÄúTouching the Earth Lightly‚Äù.pdf ...\n",
      "‚úÖ Saved compressed PDF: compressed_files/emralinolalaine_LATE_162831_14938886_Glenn Murcutt_ ‚ÄúTouching the Earth Lightly‚Äù.pdf\n",
      "üîÑ Compressing krukjulia_LATE_198551_15046680_Eero Saarinen_ Cogs 160 Research Document.pdf ...\n",
      "‚úÖ Saved compressed PDF: compressed_files/krukjulia_LATE_198551_15046680_Eero Saarinen_ Cogs 160 Research Document.pdf\n",
      "üîÑ Compressing spavenchristine_LATE_96300_14929508_COGS 160_1 (1).pdf ...\n",
      "‚úÖ Saved compressed PDF: compressed_files/spavenchristine_LATE_96300_14929508_COGS 160_1 (1).pdf\n",
      "üîÑ Compressing khirwadkarisha_166304_14925482_Report.pdf ...\n",
      "‚úÖ Saved compressed PDF: compressed_files/khirwadkarisha_166304_14925482_Report.pdf\n",
      "üîÑ Compressing liangmichael_188529_14924464_COGS 160 - Docs.pdf ...\n",
      "‚úÖ Saved compressed PDF: compressed_files/liangmichael_188529_14924464_COGS 160 - Docs.pdf\n",
      "üîÑ Compressing yangheiman_LATE_190478_14963531_COGS160.pdf ...\n",
      "‚úÖ Saved compressed PDF: compressed_files/yangheiman_LATE_190478_14963531_COGS160.pdf\n",
      "üîÑ Compressing wucynthia_LATE_167097_15019933_COGS 160.pdf ...\n",
      "‚úÖ Saved compressed PDF: compressed_files/wucynthia_LATE_167097_15019933_COGS 160.pdf\n",
      "üîÑ Compressing khirwadkarisha_166304_14925456_ken yeang cogs 160 in class.pdf ...\n",
      "‚úÖ Saved compressed PDF: compressed_files/khirwadkarisha_166304_14925456_ken yeang cogs 160 in class.pdf\n",
      "üîÑ Compressing hsucalvin_166834_14967682_Cogs 160 Slide and Doc Links for Turn-in.pdf ...\n",
      "‚úÖ Saved compressed PDF: compressed_files/hsucalvin_166834_14967682_Cogs 160 Slide and Doc Links for Turn-in.pdf\n",
      "üîÑ Compressing dasilvatheo_LATE_171930_14930244_HW A1.pdf ...\n",
      "‚úÖ Saved compressed PDF: compressed_files/dasilvatheo_LATE_171930_14930244_HW A1.pdf\n",
      "üîÑ Compressing davidmatthew_LATE_134808_14949557_COGS 160_ A1.pdf ...\n",
      "‚úÖ Saved compressed PDF: compressed_files/davidmatthew_LATE_134808_14949557_COGS 160_ A1.pdf\n",
      "üîÑ Compressing marvanalicia_212624_14925657_A.Marvan_LuisBarragan.pdf ...\n",
      "‚úÖ Saved compressed PDF: compressed_files/marvanalicia_212624_14925657_A.Marvan_LuisBarragan.pdf\n",
      "üîÑ Compressing delacruzrenier_LATE_226065_14930691_Paul Rudolph_ Life, Work, and Enduring Influence.pdf ...\n",
      "‚úÖ Saved compressed PDF: compressed_files/delacruzrenier_LATE_226065_14930691_Paul Rudolph_ Life, Work, and Enduring Influence.pdf\n",
      "üîÑ Compressing vidyalatanvi_LATE_218146_14937211_COGS 160 A1-1.pdf ...\n",
      "‚úÖ Saved compressed PDF: compressed_files/vidyalatanvi_LATE_218146_14937211_COGS 160 A1-1.pdf\n",
      "üîÑ Compressing liangmichael_188529_14924465_Michael - COGS 160.pdf ...\n",
      "‚úÖ Saved compressed PDF: compressed_files/liangmichael_188529_14924465_Michael - COGS 160.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz      # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def compress_all_pdfs(input_dir, output_dir, dpi=100, downscale_factor=2):\n",
    "    \"\"\"\n",
    "    Compress all PDF files in `input_dir` by rendering each page to an image,\n",
    "    optionally downscaling, and reassembling into a new PDF in `output_dir`.\n",
    "\n",
    "    :param input_dir: Folder containing original PDF files.\n",
    "    :param output_dir: Folder to write compressed PDFs.\n",
    "    :param dpi: DPI for rendering pages.\n",
    "    :param downscale_factor: Factor by which to downscale rendered images.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if not filename.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        print(f\"üîÑ Compressing {filename} ...\")\n",
    "        try:\n",
    "            doc = fitz.open(input_path)\n",
    "            new_pdf = fitz.open()\n",
    "\n",
    "            for page in doc:\n",
    "                pix = page.get_pixmap(dpi=dpi)\n",
    "                # create PIL image from raw samples\n",
    "                img = Image.frombytes(\"RGB\", (pix.width, pix.height), pix.samples)\n",
    "\n",
    "                # downscale using LANCZOS filter\n",
    "                new_size = (pix.width // downscale_factor, pix.height // downscale_factor)\n",
    "                img = img.resize(new_size, Image.LANCZOS)\n",
    "\n",
    "                # save into a one-page PDF in memory\n",
    "                buffer = io.BytesIO()\n",
    "                img.save(buffer, format=\"PDF\", resolution=dpi)\n",
    "                buffer.seek(0)\n",
    "\n",
    "                img_pdf = fitz.open(\"pdf\", buffer)\n",
    "                new_pdf.insert_pdf(img_pdf)\n",
    "\n",
    "            new_pdf.save(output_path)\n",
    "            new_pdf.close()\n",
    "            doc.close()\n",
    "            print(f\"‚úÖ Saved compressed PDF: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to compress {filename}: {e}\")\n",
    "\n",
    "\n",
    "INPUT_DIR  = \"submissions\"\n",
    "OUTPUT_DIR = \"compressed_files\"\n",
    "\n",
    "compress_all_pdfs(\n",
    "    input_dir=INPUT_DIR,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    dpi=100,\n",
    "    downscale_factor=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Unmatched login_ids: ['mainayardaniel' 'emralinolalaine' 'krukjulia' 'spavenchristine'\n",
      " 'khirwadkarisha' 'liangmichael' 'yangheiman' 'dasilvatheo' 'davidmatthew'\n",
      " 'marvanalicia' 'vidyalatanvi']\n",
      "\n",
      "‚úÖ Done! Check ‚Üí image_metadata_with_name_pid.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# ‚îÄ‚îÄ 1) Load CSVs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "meta_df   = pd.read_csv(\"image_metadata.csv\")\n",
    "roster_df = pd.read_csv(\"student_info.csv\")\n",
    "\n",
    "# ‚îÄ‚îÄ 2) Extract login_id from your PDF filenames ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "#    e.g. \"mainayardaniel_127050_14924649_COGS160‚Ä¶\" ‚Üí \"mainayardaniel\"\n",
    "meta_df[\"login_id\"] = meta_df[\"pdf_file\"].str.split(\"_\").str[0]\n",
    "\n",
    "# ‚îÄ‚îÄ 3) Tidy roster columns ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "roster_df = roster_df.rename(columns={\n",
    "    \"SIS Login ID\": \"login_id\",\n",
    "    \"Student\":      \"student_name\",\n",
    "    \"SIS User ID\":  \"pid\"\n",
    "})\n",
    "# ensure no NaNs and strip whitespace\n",
    "roster_df[\"student_name\"] = roster_df[\"student_name\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "# ‚îÄ‚îÄ 4) Exact‚Äêmatch merge ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "merged = pd.merge(\n",
    "    meta_df,\n",
    "    roster_df[[\"login_id\",\"student_name\",\"pid\"]],\n",
    "    on=\"login_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ 5) Which login_ids still have no PID? ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "unmatched = merged.loc[merged[\"pid\"].isna(), \"login_id\"].unique()\n",
    "print(\"üîç Unmatched login_ids:\", unmatched)\n",
    "\n",
    "# ‚îÄ‚îÄ 6) Prepare for fuzzy matching ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "#    create a normalized key (lowercase, no punctuation/space)\n",
    "roster_df[\"norm_name\"] = (\n",
    "    roster_df[\"student_name\"]\n",
    "      .str.lower()\n",
    "      .str.replace(r\"[^a-z0-9]\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "# map norm_name ‚Üí (login_id, student_name, pid)\n",
    "roster_map = {\n",
    "    row.norm_name: (row.login_id, row.student_name, row.pid)\n",
    "    for row in roster_df.itertuples()\n",
    "}\n",
    "\n",
    "# ‚îÄ‚îÄ 7) Build fuzzy‚Äêmatch suggestions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "suggestions = {}\n",
    "for uid in unmatched:\n",
    "    key = str(uid).lower()\n",
    "    # 7a) exact normalized-name\n",
    "    if key in roster_map:\n",
    "        suggestions[uid] = [roster_map[key]]\n",
    "        continue\n",
    "    # 7b) substring match\n",
    "    hits = [roster_map[n] for n in roster_map if key in n or n in key]\n",
    "    if hits:\n",
    "        suggestions[uid] = hits\n",
    "        continue\n",
    "    # 7c) difflib fallback\n",
    "    best = get_close_matches(key, roster_map.keys(), n=1, cutoff=0.6)\n",
    "    suggestions[uid] = [roster_map[best[0]]] if best else []\n",
    "\n",
    "# ‚îÄ‚îÄ 8) Auto-fill those with exactly one candidate ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "for uid, matches in suggestions.items():\n",
    "    if len(matches) == 1:\n",
    "        _, name, pid = matches[0]\n",
    "        merged.loc[merged[\"login_id\"] == uid, \"student_name\"] = name\n",
    "        merged.loc[merged[\"login_id\"] == uid, \"pid\"]          = pid\n",
    "\n",
    "# ‚îÄ‚îÄ 9) Print any truly ambiguous or missing cases ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "for uid, matches in suggestions.items():\n",
    "    if len(matches) > 1:\n",
    "        print(f\"\\n‚ö†Ô∏è {uid!r} HAS MULTIPLE MATCHES:\")\n",
    "        for _, name, pid in matches:\n",
    "            print(f\"    {name} ‚Üí {pid}\")\n",
    "    elif not matches:\n",
    "        print(f\"\\n‚ùå {uid!r} HAS NO CLOSE MATCH\")\n",
    "\n",
    "# ‚îÄ‚îÄ10) Save the finished CSV ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "out_path = \"image_metadata_with_name_pid.csv\"\n",
    "merged.to_csv(out_path, index=False)\n",
    "print(f\"\\n‚úÖ Done! Check ‚Üí {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XR_Lab (py3.11)",
   "language": "python",
   "name": "xr_lab_311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
